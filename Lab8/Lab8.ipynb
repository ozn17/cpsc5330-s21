{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "3cad7123-c888-48a6-81ae-91543e02d59c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Streaming Log Processing\n",
    "\n",
    "This is a simple exercise in log processing.  The log files come from various servers at various time points.\n",
    "Each record in a log file is of the form\n",
    "\n",
    "```serverID,severity,timestamp```\n",
    "\n",
    "* serverID is a string unique to the server\n",
    "* severity is a value 2 (no error, just a service call), or 1 (minor error), or 0 (fatal/severe error)\n",
    "* timestamp is an integer starting at 1 (bigger numbers mean later)\n",
    "\n",
    "For this exercise these log files will be \"delivered\" by being placed in a directory, for example ```/FileStore/tables/logdata-live```.\n",
    "The log files for this small example have two servers, s1 and s2 and log records for times 1 through 10.\n",
    "The files are delivered with one file per server for five time units.  For example, the file s115.csv has records for server 1 for times 1 through 5.\n",
    "\n",
    "You want to process these new records incrementally, and are interested in these two \"reports\"\n",
    "\n",
    "*  The *volume report* reports by server the number of SEV2 events divided by the number of time units.  The number of time units for our purposes is (max(timestamp) - min(timestamp)) + 1.  This volume report will not be cumulative -- i.e. every time new log data comes in, the mapping from server to sev2 volume is updated\n",
    "* The *sev0 log* -- this is a sequence of records of the form ```serverID timestamp``` recording a SEV0 event reported by the server.  This report grows over time -- each time a new log file is processed, new records are appended to the end.\n",
    "\n",
    "Your final result should be two streaming queries\n",
    "* One that *modifies* the volume report, which is stored in memory\n",
    "* One that *appends to* the sev0 log, which is stored as a Parquet file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "2e0b85e3-2fe4-4cec-8f56-2a9be19d3b56",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Verify that there are four log files in your staging directory\n",
    "# If using EMR notebook, do this at the AWS console and leave this cell blank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "5922fb2d-de2f-4c19-bbe5-1c8849d2794e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create the directory that will hold your \"live streaming files\"\n",
    "# If using EMR notebook, do this at the AWS console and leave this cell blank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "34ff72b0-61d4-4e92-b478-c0c84dc9c375",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#  Create the schema for the log files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "a513f133-14fd-4e46-bba8-855f401aadc0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create the streaming DataFrame (readStream) on your log directory, using the schema you just created\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "1e4dd595-b686-4ba0-9baf-3326c63995f5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Use the data frame you just created to create another data frame with the \n",
    "# sev2 volume report.  It should have columns 'serverID' and 'avgVolume'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "ad420b2b-d748-4c5b-b568-5369c44c9396",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create and start a query (writeStream) that generates the sev2 report;  it is an in-memory sink.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "5a44bb60-19c9-4f50-9278-641fb4806ca2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Write a (very simple) spark SQL query to show the contents of your query.  It should initially be empty\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "6cd1418b-7ed3-4713-a5b7-7f8162fbfa2a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Copy two files into your 'live data' directory for both servers, for both servers and time period 1 through 5\n",
    "# If using EMR notebook, do this at the AWS console and leave this cell blank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "f245ecc8-1a90-4a36-9c2d-c1d25b5a2827",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Rerun the query to show that the sev2 volume report has been updated\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "d0589690-9a47-49ac-b303-44e6fad7da49",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Now copy the log files for times 6 to 10\n",
    "# If using EMR notebook, do this at the AWS console and leave this cell blank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "c4246b60-8de0-4484-b6f3-274eb1ca3ad5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Run the query again to verify that the report was updated. Be sure to wait for a little while\n",
    "# to make sure the query is updated.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "8b8e2417-e879-45bf-a99e-bedb89fd9e81",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### The SEV0 log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "35611c93-c48c-4b77-96d9-02e923c5585f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Delete all files from your \"live\" directory\n",
    "# If using EMR notebook, do this at the AWS console and leave this cell blank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "744e269d-0f08-4634-aca5-bd9d4aac8741",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create a data frame on top of your original data frame that holds the raw data, \n",
    "# this data frame for the sev0 report is just <serverID> <time stamp>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "4e9bb771-f5ba-4295-be8a-061363ab8eb5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create a query on your sev0 data frame that writes the table to a parquet file, \n",
    "#  appending new records to the file\n",
    "# https://stackoverflow.com/questions/55859868/pyspark-structured-streaming-write-to-parquet-in-batches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "a5766357-c0ca-434b-83e7-29824ee68976",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Display the query content by reading the parquet file (it should be empty)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "4aa8b252-8528-43ae-a8b6-0efa07c2e54b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Copy in the files for timestamp 1 through 5\n",
    "# If using EMR notebook, do this at the AWS console and leave this cell blank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "118d2643-5e50-4119-aa20-eb61d4c2ef5c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Display the query again by reading the parquet file.  Are there new records?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "1b799364-f529-48d3-9557-56556433fde3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Copy in the files for timestamp 6 through 10\n",
    "# If using EMR notebook, do this at the AWS console and leave this cell blank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "a7b834a6-3bde-4951-96a5-8499cb7c63a0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Display the query again by reading the parquet file.  Are there new records?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "ca26c769-6346-4116-9c53-d8d2b0ecb63a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Be tidy, stop all your streaming queries!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "252d36a3-6ccb-45d1-9a15-ddd1a1afd279",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Verify that there are no active streams\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "StreamingLab",
   "notebookOrigID": 3258061014311749,
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "name": "StreamingLab",
  "notebookId": 3750749627479206
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
