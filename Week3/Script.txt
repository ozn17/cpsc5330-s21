Agenda for 4/13

1. Review the "average population per country scenario"
2. So far
  -- Imported a database into MySQL
  -- Performed a query to get the names of the 10 countries that have highest population per city, in order

MySQL
show tables;
describe City;
select CountryCode, avg(Population) from City group by CountryCode order by avg(Population) desc limit 10;
[[ Put a join here on country name]]

-- make the /database/world directories

sqoop import --connect jdbc:mysql://localhost/world \
 --username training --password training \
 --table City \
 --target-dir /database/world/City

Look at the MapReduce code, which extracts country code and population, then Hadoop does a 
group, and the reducer takes the average.   We didn't do sorting, though you saw in the lab how to do it.
We didn't do limiting, and that's even more problematic completely w/in 

Run the code 
hdfs dfs -mkdir /output
hadoop-streaming country-avg-pop /database/world/City /output/City

hdfs dfs -getmerge /data-output/city city-output.tsv

   and to directly compare to SQL:

      sort city-output.tsv -k 2 -n -r | head -n 10
      select CountryCode, avg(Population) from City group by CountryCode order by avg(Population) desc limit 10;

7.  And finally we need to do a Sqoop export to get this "tabular" data back into mysql

create table population_by_country_code (country_code char(3), average_population float(10,2));

sqoop export --connect jdbc:mysql://localhost/world 
   --table population_by_country_code 
   --export-dir /data-output/City 
   --username training 
   --password training 
   --input-fields-terminated-by '\t'

select * from population_by_country_code order by average_population desc limit 10;

===============
Hive

Now remember we have our World db, and we already calculated the codes of the countries 
with the highest city-level population density (person per city).  Simple extra step:  join 
with the Country table to get the country name.

1.  Desired output through query to  MySQL

select Country.Name, avg(City.Population) from City, Country where City.CountryCode = Country.Code group by CountryCode order by avg(City.Population) desc limit 10;

2.  We'll create the two Hive tables, City and Country.  One we will do from the current 
    HDFS files, the other we will do directly from MySQL.

Hive shell

 create table City (ID INT, Name STRING, CountryCode STRING, District STRING, Population INT);
 
First just create a table, then look at the metastore

 SELECT c.* FROM TBLS t
 JOIN DBS d
 ON t.DB_ID = d.DB_ID
 JOIN SDS s
 ON t.SD_ID = s.SD_ID
 JOIN COLUMNS_V2 c
 ON s.CD_ID = c.CD_ID
 WHERE TBL_NAME = 'city'
 AND d.NAME='default'

Then associate the data stream with it.
 load data inpath "/user/training/City" into table city;

But do a select * from city and see that it didn't get the field delimiter.

    drop table City

    create table City (ID INT, Name STRING, CountryCode STRING, District STRING, Population INT) row format delimited fields terminated by ',';
    load data inpath "/user/training/City" into table city;
   After that completes, note that the data is here:  /user/hive/warehouse/city

6.  Now try to get Country into Hive using sqoop

     sqoop import --bindir /tmp \
	--connect jdbc:mysql://localhost/world \
	--username training --password training \
	--fields-terminated-by '\t' \
	--table Country \
	--hive-import

   Remember Country needs to be deleted in HDFS ahead of time.
   Verify the table and its schema.

7.  Now the join:
    The first join I tried worked in MySQL 

    select Country.Name, avg(City.Population) from City, Country where City.CountryCode = Country.Code group by CountryCode order by avg(City.Population) desc limit 10;

So separate the join from the group using a subquery and it works:

    select Country.Name, pop.ap from Country, (select Countrycode as cc, avg(Population) as ap from City group by CountryCode) as pop where Country.code = cc;

8.  And now finally we need to export the Hive table out to the local filesystem:

INSERT OVERWRITE LOCAL DIRECTORY '/home/training/country-average-population' \
	ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t' \
	STORED AS TEXTFILE \
	SELECT Country.Name, pop.ap from Country, (select Countrycode as cc, avg(Population) as ap from City group by CountryCode) as pop where Country.code = cc;


!! Check on whether we can do sort and limit here
(It's a known hive bug that you can't use a custom separator to write to HDFS.  That's why we go local.)

And finally we can do this:  IF NECESSARY!

cat country-average-population/* | sort -k 2 -t $'\t' -n -r | head -n 10

Summary:
Using Hadoop / HDFS look like an RDBMS
1.  Load relational data into HDFS using Sqoop
2.  Creating tables in Hive
3.  Associating a table with data in HDFS
4.  Hive queries
5.  Exporting

A big deal right now is schema information.  
Here we have the schema information associated with the Hive Metastore (only) but when we export out 
to the next app or the next phase, that information is lost.  The next step will be to see how we can associate
schema information with the data itself.






